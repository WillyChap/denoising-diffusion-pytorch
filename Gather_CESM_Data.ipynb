{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60385a9-8677-4cd6-9799-642e977025c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import os \n",
    "import numpy as np \n",
    "import sys \n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms as T, utils\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from denoising_diffusion_pytorch.attend import Attend\n",
    "\n",
    "from denoising_diffusion_pytorch.version import __version__\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5bbf6-8b14-4670-be1a-a5d080d1eaf8",
   "metadata": {},
   "source": [
    "## Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bb11a-fc8a-4a0c-8cd0-7b3d9d722c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "gpu_id=0\n",
    "\"\"\n",
    "def set_gpu(gpu_id):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    \n",
    "if gpu_id >= 0:\n",
    "    device = \"cuda\"\n",
    "    set_gpu(gpu_id)\n",
    "    print('device available :', torch.cuda.is_available())\n",
    "    print('device count: ', torch.cuda.device_count())\n",
    "    print('current device: ',torch.cuda.current_device())\n",
    "    print('device name: ',torch.cuda.get_device_name())\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a1ac3-fe31-47a6-8489-0bfd38b6f467",
   "metadata": {},
   "source": [
    "## Find all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975335a-7ef3-4f27-927e-b4ee8eaef7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_root = '/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/month_1/'\n",
    "\n",
    "vars = ['PS','PRECT','TREFHT']\n",
    "bingo = 0\n",
    "FNS1_big = []\n",
    "FNS2_big = []\n",
    "FNS3_big = []\n",
    "for yryr in range(1850,2020,10):\n",
    "    for ii in range(1,11):\n",
    "        for gogo in ['1001','1021','1041','1061','1081','1101','1121','1141','1161','1181']:\n",
    "            FNS1 = glob.glob(f'{fpath_root}/{vars[0]}/*-{gogo}*.{ii:03}.*{yryr}*')\n",
    "            FNS2 = glob.glob(f'{fpath_root}/{vars[1]}/*-{gogo}*.{ii:03}.*{yryr}*')\n",
    "            FNS3 = glob.glob(f'{fpath_root}/{vars[2]}/*-{gogo}*.{ii:03}.*{yryr}*')\n",
    "            if len(FNS1)>0:\n",
    "                break\n",
    "        bingo+=len(FNS1)\n",
    "\n",
    "        if len(FNS1)>1:\n",
    "            print('oh no')\n",
    "        FNS1_big.append(FNS1[0])\n",
    "        FNS2_big.append(FNS2[0])\n",
    "        FNS3_big.append(FNS3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e6dcf-8034-435f-920c-375b8c878af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpathO = '/glade/derecho/scratch/wchapman/CESM_LE2_vars/' \n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(dpathO):\n",
    "    os.makedirs(dpathO)\n",
    "    print(f\"Directory {dpathO} created.\")\n",
    "else:\n",
    "    print(f\"Directory {dpathO} already exists.\")\n",
    "    \n",
    "for ii in range(len(FNS1_big)):\n",
    "\n",
    "    f1lis = FNS1_big[ii].split('/')[-1].split(f'{vars[0]}')\n",
    "    f2lis = FNS2_big[ii].split('/')[-1].split(f'{vars[1]}')\n",
    "    f3lis = FNS3_big[ii].split('/')[-1].split(f'{vars[2]}')\n",
    "\n",
    "    out_fil = f'{dpathO}{f1lis[0]}{vars[0]}.{vars[1]}.{vars[2]}{f1lis[1]}'\n",
    "\n",
    "    if os.path.exists(out_fil):\n",
    "        print('skipped')\n",
    "        continue\n",
    "\n",
    "    DS1 = xr.open_dataset(FNS1_big[ii])\n",
    "    DS2 = xr.open_dataset(FNS2_big[ii])\n",
    "    DS3 = xr.open_dataset(FNS3_big[ii])\n",
    "\n",
    "    if f1lis == f2lis == f3lis:\n",
    "        DS = xr.merge([DS1,DS2,DS3])[vars]\n",
    "        DS.to_netcdf(out_fil)\n",
    "        print(f'made {out_fil}')\n",
    "    else:\n",
    "        print('didnt meet criteria')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29681fc4-b8a7-4379-8edd-9892fd7f86d5",
   "metadata": {},
   "source": [
    "## Get mean and std and max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f5ff0-23b2-4c93-a369-b1061ce0430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSall = xr.open_mfdataset(sorted(glob.glob('/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.001.*.nc')))\n",
    "# DSall.mean().to_array().values\n",
    "\n",
    "m_1 = []\n",
    "m_2 = []\n",
    "m_3 = []\n",
    "s_1 = []\n",
    "s_2 = []\n",
    "s_3 = []\n",
    "for ii in range(1,11):\n",
    "    DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.{ii:03}.*.nc')))\n",
    "    mv = DSall.mean(['time']).to_array().values\n",
    "    sv = DSall.std(['time']).to_array().values\n",
    "    m_1.append(mv[0])\n",
    "    m_2.append(mv[1])\n",
    "    m_3.append(mv[2])\n",
    "\n",
    "    s_1.append(sv[0])\n",
    "    s_2.append(sv[1])\n",
    "    s_3.append(sv[2])\n",
    "    print(ii)\n",
    "    \n",
    "dicter = {'PS_mean':np.array(m_1).mean(axis=0),'PRECT_mean':np.array(m_2).mean(axis=0),'TREFHT_mean':np.array(m_3).mean(axis=0),\n",
    "          'PS_std':np.array(s_1).mean(axis=0),'PRECT_std':np.array(s_2).mean(axis=0),'TREFHT_std':np.array(s_3).mean(axis=0)}\n",
    "# Save to a file\n",
    "with open('scaling_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(dicter, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c5357-dc26-417c-88e4-fc573d69f129",
   "metadata": {},
   "source": [
    "## Min Max of Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423876c-c658-4329-bacb-5bdb3117d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the file\n",
    "\n",
    "ma_1 = []\n",
    "ma_2 = []\n",
    "ma_3 = []\n",
    "mi_1 = []\n",
    "mi_2 = []\n",
    "mi_3 = []\n",
    "\n",
    "with open('scaling_dict.pkl', 'rb') as file:\n",
    "    loaded_dicter = pickle.load(file)\n",
    "\n",
    "for ii in range(1,11):\n",
    "    print(ii)\n",
    "    DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.{ii:03}.*.nc')))\n",
    "    DSall['PS'][:,:,:] = (DSall['PS']-loaded_dicter['PS_mean'])/loaded_dicter['PS_std']\n",
    "\n",
    "    DSall['PRECT'][:,:,:] = (DSall['PRECT']-loaded_dicter['PRECT_mean'])/loaded_dicter['PRECT_std']\n",
    "\n",
    "    DSall['TREFHT'][:,:,:] = (DSall['TREFHT']-loaded_dicter['TREFHT_mean'])/loaded_dicter['TREFHT_std']\n",
    "\n",
    "    ma_1.append(np.max(DSall['PS']).values)\n",
    "    mi_1.append(np.min(DSall['PS']).values)\n",
    "    ma_2.append(np.max(DSall['PRECT']).values)\n",
    "    mi_2.append(np.min(DSall['PRECT']).values)\n",
    "    ma_3.append(np.max(DSall['TREFHT']).values)\n",
    "    mi_3.append(np.min(DSall['TREFHT']).values)\n",
    "\n",
    "dicter = {'PS_max':np.array(ma_1).max(),'PRECT_max':np.array(ma_2).max(),'TREFHT_max':np.array(ma_3).max(),\n",
    "          'PS_min':np.array(mi_1).min(),'PRECT_min':np.array(mi_2).min(),'TREFHT_min':np.array(mi_3).min()}\n",
    "# Save to a file\n",
    "with open('scaling_dict_minmax.pkl', 'wb') as file:\n",
    "    pickle.dump(dicter, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69942e-9c3c-40ad-8451-2096150b9aed",
   "metadata": {},
   "source": [
    "## Climate Change Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec0c0d-94b7-4b15-938d-dd6949285f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_root = '/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/month_1/'\n",
    "vars = ['CO2']\n",
    "FNS4_big = []\n",
    "for yryr in range(2015,2095,10):\n",
    "    for ii in range(1,11):\n",
    "        FNS4 = glob.glob(f'{fpath_root}/{vars[0]}/*BSSP370cmip6*.{ii:03}.*{yryr}*')\n",
    "\n",
    "        for fn in FNS4:\n",
    "            FNS4_big.append(fn)\n",
    "\n",
    "\n",
    "def pres_from_hybrid(psfc, hya, hyb, p0=100000.):\n",
    "    \"\"\"Calculates pressure field\n",
    "\n",
    "    pressure derived with the formula:\n",
    "    ```p = a(k)*p0 + b(k)*ps```\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    psfc\n",
    "        surface pressure\n",
    "    hya, hyb\n",
    "        hybrid-sigma A and B coefficients\n",
    "    p0 : optional\n",
    "        reference pressure, defaults to 100000 Pa\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pressure, size is same as `psfc` with `len(hya)` levels\n",
    "    \"\"\"\n",
    "    return hya*p0 + hyb*psfc\n",
    "\n",
    "\n",
    "def vertical_integral(fld, ps, acoef, bcoef):\n",
    "    \"\"\"Calculate weighted vertical average using trapezoidal rule. Uses full column.\"\"\"\n",
    "    pres = pres_from_hybrid(ps, acoef, bcoef)\n",
    "    # integral of del_pressure turns out to be just the average of the square of the boundaries:\n",
    "    # -- assume lev is a coordinate and is nominally in pressure units\n",
    "    maxlev = pres['lev'].max().item()\n",
    "    minlev = pres['lev'].min().item()\n",
    "    dp_integrated = 0.5 * (pres.sel(lev=maxlev)**2 - pres.sel(lev=minlev)**2)\n",
    "    levaxis = fld.dims.index('lev')  # fld needs to be a dataarray\n",
    "    assert isinstance(levaxis, int), f'the axis called lev is not an integer: {levaxis}'\n",
    "    fld_integrated = np.trapz(fld * pres, x=pres, axis=levaxis)\n",
    "    return fld_integrated/(dp_integrated/dp_integrated)\n",
    "\n",
    "\n",
    "for ee,fnfn in enumerate(FNS4_big):\n",
    "\n",
    "    fout = f'/glade/derecho/scratch/wchapman/CESM_LE2_vars_BSSP370cmip6/{fnfn.split(\"/\")[-1].replace(\"CO2\",\"CO2_PRECT_TREFHT_PS\")}'\n",
    "    print(f'doing loop {ee:04} of {len(FNS4_big)}')\n",
    "\n",
    "    DS_co2 = xr.open_dataset(fnfn)\n",
    "    DS_prect = xr.open_dataset(fnfn.replace('CO2', 'PRECT'))\n",
    "    DS_trefht = xr.open_dataset(fnfn.replace('CO2', 'TREFHT'))\n",
    "    DS_ps = xr.open_dataset(fnfn.replace('CO2', 'PS'))\n",
    "\n",
    "    \n",
    "    DA_co2 = DS_co2['CO2']\n",
    "    acoef = DS_co2['hyam']\n",
    "    bcoef = DS_co2['hybm']\n",
    "    DA_ps = DS_ps['PS']\n",
    "\n",
    "    concat_da = []\n",
    "    for ii in range(len(DA_ps['time'])):\n",
    "        # Perform the vertical integral\n",
    "        va = vertical_integral(DA_co2[ii, :, :, :].squeeze(), DA_ps[ii, :, :].squeeze(), acoef, bcoef)\n",
    "        \n",
    "        # Create a new DataArray with expanded time dimension\n",
    "        va = va.expand_dims({'time': [DA_ps['time'][ii].item()]})\n",
    "        \n",
    "        # Convert to dataset and append\n",
    "        concat_da.append(va.to_dataset(name='VI_CO2'))\n",
    "\n",
    "    # Merge the datasets along the time dimension\n",
    "    final_DA = xr.merge(concat_da)\n",
    "\n",
    "    DSout = xr.merge([final_DA,DS_prect,DS_trefht,DS_ps])\n",
    "\n",
    "    fnfn.split('/')[-1].replace('CO2',f'CO2_PRECT_TREFHT_PS')\n",
    "\n",
    "    DSout.to_netcdf(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f0d53-a349-4025-b918-9194f7881f00",
   "metadata": {},
   "source": [
    "## Find Scaling Terms while preserving std and mean pooling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26898d39-a1b8-40c1-8673-37410733f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file set 1\n",
      "Processed file set 2\n",
      "Processed file set 3\n",
      "Processed file set 4\n",
      "Processed file set 5\n",
      "Processed file set 6\n",
      "Processed file set 7\n",
      "Processed file set 8\n",
      "Processed file set 9\n",
      "Processed file set 10\n",
      "Results saved to scaling_dict_CC.pkl\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Initialize lists for means, standard deviations, and sample sizes\n",
    "means_1, means_2, means_3 = [], [], []\n",
    "stds_1, stds_2, stds_3 = [], [], []\n",
    "counts_1, counts_2, counts_3 = [], [], []  # To store sample sizes\n",
    "\n",
    "for ii in range(1, 11):\n",
    "    # Open the dataset\n",
    "    DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars_BSSP370cmip6/*.{ii:03}.*.nc')))[['PS', 'PRECT', 'TREFHT']]\n",
    "    \n",
    "    # Calculate mean and std over time\n",
    "    mean_values = DSall.mean(['time']).to_array().values\n",
    "    std_values = DSall.std(['time']).to_array().values\n",
    "    \n",
    "    # Get the number of time points (for weighting)\n",
    "    count = len(DSall['time'])\n",
    "    \n",
    "    # Append means, stds, and counts for weighting\n",
    "    means_1.append(mean_values[0])\n",
    "    means_2.append(mean_values[1])\n",
    "    means_3.append(mean_values[2])\n",
    "    \n",
    "    stds_1.append(std_values[0])\n",
    "    stds_2.append(std_values[1])\n",
    "    stds_3.append(std_values[2])\n",
    "    \n",
    "    counts_1.append(count)\n",
    "    counts_2.append(count)\n",
    "    counts_3.append(count)\n",
    "\n",
    "    print(f'Processed file set {ii}')\n",
    "    \n",
    "# Convert to numpy arrays for easier manipulation\n",
    "means_1, means_2, means_3 = np.array(means_1), np.array(means_2), np.array(means_3)\n",
    "stds_1, stds_2, stds_3 = np.array(stds_1), np.array(stds_2), np.array(stds_3)\n",
    "counts_1, counts_2, counts_3 = np.array(counts_1), np.array(counts_2), np.array(counts_3)\n",
    "\n",
    "counts_1 = counts_1[:, np.newaxis, np.newaxis]  # Shape (10, 1, 1) for broadcasting\n",
    "counts_2 = counts_2[:, np.newaxis, np.newaxis]\n",
    "counts_3 = counts_3[:, np.newaxis, np.newaxis]\n",
    "\n",
    "# Step 1: Calculate the weighted combined mean for each variable\n",
    "combined_mean_1 = np.sum(counts_1 * means_1, axis=0) / np.sum(counts_1, axis=0)\n",
    "combined_mean_2 = np.sum(counts_2 * means_2, axis=0) / np.sum(counts_2, axis=0)\n",
    "combined_mean_3 = np.sum(counts_3 * means_3, axis=0) / np.sum(counts_3, axis=0)\n",
    "\n",
    "# Step 2: Calculate the combined variance for each variable\n",
    "combined_var_1 = (np.sum(counts_1 * (stds_1**2 + means_1**2), axis=0) / np.sum(counts_1, axis=0)) - combined_mean_1**2\n",
    "combined_var_2 = (np.sum(counts_2 * (stds_2**2 + means_2**2), axis=0) / np.sum(counts_2, axis=0)) - combined_mean_2**2\n",
    "combined_var_3 = (np.sum(counts_3 * (stds_3**2 + means_3**2), axis=0) / np.sum(counts_3, axis=0)) - combined_mean_3**2\n",
    "\n",
    "# Step 3: Calculate the combined standard deviation by taking the square root of the variance\n",
    "combined_std_1 = np.sqrt(combined_var_1)\n",
    "combined_std_2 = np.sqrt(combined_var_2)\n",
    "combined_std_3 = np.sqrt(combined_var_3)\n",
    "\n",
    "# Create dictionary to store results\n",
    "dicter = {\n",
    "    'PS_mean': combined_mean_1,\n",
    "    'PRECT_mean': combined_mean_2,\n",
    "    'TREFHT_mean': combined_mean_3,\n",
    "    'PS_std': combined_std_1,\n",
    "    'PRECT_std': combined_std_2,\n",
    "    'TREFHT_std': combined_std_3\n",
    "}\n",
    "\n",
    "# Save to a file\n",
    "with open('scaling_dict_CC.pkl', 'wb') as file:\n",
    "    pickle.dump(dicter, file)\n",
    "\n",
    "print(\"Results saved to scaling_dict_CC.pkl\")\n",
    "\n",
    "with open('scaling_dict_CC.pkl', 'rb') as file:\n",
    "    loaded_mean_std_dict_CC_fix = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e79f27de-3295-4558-ae63-c4539cfdee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Load from the file\n",
    "ma_1 = []\n",
    "ma_2 = []\n",
    "ma_3 = []\n",
    "mi_1 = []\n",
    "mi_2 = []\n",
    "mi_3 = []\n",
    "\n",
    "with open('scaling_dict_CC.pkl', 'rb') as file:\n",
    "    loaded_dicter = pickle.load(file)\n",
    "\n",
    "for ii in range(1,11):\n",
    "    print(ii)\n",
    "    DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars_BSSP370cmip6/*.{ii:03}.*.nc')))\n",
    "    DSall['PS'][:,:,:] = (DSall['PS']-loaded_dicter['PS_mean'])/loaded_dicter['PS_std']\n",
    "\n",
    "    DSall['PRECT'][:,:,:] = (DSall['PRECT']-loaded_dicter['PRECT_mean'])/loaded_dicter['PRECT_std']\n",
    "\n",
    "    DSall['TREFHT'][:,:,:] = (DSall['TREFHT']-loaded_dicter['TREFHT_mean'])/loaded_dicter['TREFHT_std']\n",
    "\n",
    "    ma_1.append(np.max(DSall['PS']).values)\n",
    "    mi_1.append(np.min(DSall['PS']).values)\n",
    "    ma_2.append(np.max(DSall['PRECT']).values)\n",
    "    mi_2.append(np.min(DSall['PRECT']).values)\n",
    "    ma_3.append(np.max(DSall['TREFHT']).values)\n",
    "    mi_3.append(np.min(DSall['TREFHT']).values)\n",
    "\n",
    "dicter = {'PS_max':np.array(ma_1).max(),'PRECT_max':np.array(ma_2).max(),'TREFHT_max':np.array(ma_3).max(),\n",
    "          'PS_min':np.array(mi_1).min(),'PRECT_min':np.array(mi_2).min(),'TREFHT_min':np.array(mi_3).min()}\n",
    "# Save to a file\n",
    "with open('scaling_dict_minmax_CC.pkl', 'wb') as file:\n",
    "    pickle.dump(dicter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcab57e-e445-49a6-b83a-b82c78651ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"input_channels\": 1,\n",
    "        \"output_channels\": 1,\n",
    "        \"context_image\": True,\n",
    "        \"context_channels\": 1,\n",
    "        \"num_blocks\": [2, 2],\n",
    "        \"hidden_channels\": 32,\n",
    "        \"hidden_context_channels\": 8,\n",
    "        \"time_embedding_dim\": 256,\n",
    "        \"image_size\": 128,\n",
    "        \"noise_sampling_coeff\": 0.85,\n",
    "        \"denoise_time\": 970,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm\": True,\n",
    "        \"subsample\": 100000,\n",
    "        \"save_name\": \"model_weights.pt\",\n",
    "        \"dim_mults\": [4, 4],\n",
    "        \"base_dim\": 32,\n",
    "        \"timesteps\": 1000,\n",
    "        \"pading\": \"reflect\",\n",
    "        \"scaling\": \"std\",\n",
    "        \"optimization\": {\n",
    "            \"epochs\": 400,\n",
    "            \"lr\": 0.01,\n",
    "            \"wd\": 0.05,\n",
    "            \"batch_size\": 32,\n",
    "            \"scheduler\": True\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacb748-754c-4816-93a6-750e6b830632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessed(Dataset):\n",
    "    def __init__(self, file_paths, config, mean_std_dict, min_max_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of paths to netCDF files\n",
    "            config: Configuration dict with scaling and augmentation options\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.config = config\n",
    "        self.loaded_mean_std_dict = mean_std_dict\n",
    "        self.loaded_min_max_dict = min_max_dict\n",
    "        self.augmentation = config.get('augment', False)\n",
    "        self.scaler = None\n",
    "        if config['scaling'] == 'quantile':\n",
    "            self.scaler = QuantileTransformer()\n",
    "\n",
    "        # Initialize cache size (you can adjust it depending on memory constraints)\n",
    "        self.cache_size = config.get('cache_size', 10)\n",
    "        self.cached_data = {}  # Manual cache for storing loaded data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate total number of samples across all files\n",
    "        total_len = 0\n",
    "        for path in self.file_paths:\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                total_len += ds.sizes['time']  # Assuming 'time' is the main dimension\n",
    "        return total_len\n",
    "\n",
    "    def _apply_scaling(self, ds):\n",
    "        if config['scaling'] == 'std':\n",
    "            ds['PS'][:,:,:] = (ds['PS'][:,:,:] - self.loaded_mean_std_dict['PS_mean']) / self.loaded_mean_std_dict['PS_std']\n",
    "            ds['PRECT'][:,:,:] = (ds['PRECT'][:,:,:] - self.loaded_mean_std_dict['PRECT_mean']) / self.loaded_mean_std_dict['PRECT_std']\n",
    "            ds['TREFHT'][:,:,:] = (ds['TREFHT'][:,:,:] - self.loaded_mean_std_dict['TREFHT_mean']) / self.loaded_mean_std_dict['TREFHT_std']\n",
    "    \n",
    "            vars = ['PS','PRECT','TREFHT']\n",
    "    \n",
    "            for vv in vars:\n",
    "                mini = self.loaded_min_max_dict[f'{vv}_min']\n",
    "                maxi = self.loaded_min_max_dict[f'{vv}_max']\n",
    "                if maxi > 10:\n",
    "                    maxi = 10\n",
    "                if mini <-10:\n",
    "                    mini = 10\n",
    "                \n",
    "                ds[vv][:,:,:] =  (ds[vv][:,:,:] - mini) / (maxi - mini)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method specified.\")\n",
    "    \n",
    "        return ds\n",
    "\n",
    "    def _augment_data(self, data):\n",
    "        # Apply rotations for augmentation\n",
    "        data_rot90 = np.rot90(data, k=1, axes=(1, 2))\n",
    "        data_rot180 = np.rot90(data, k=2, axes=(1, 2))\n",
    "        data_rot270 = np.rot90(data, k=3, axes=(1, 2))\n",
    "        \n",
    "        # Concatenate all rotations along the first dimension (time)\n",
    "        return np.concatenate([data, data_rot90, data_rot180, data_rot270], axis=0)\n",
    "\n",
    "    @lru_cache(maxsize=4)  # Cache up to 5 file loads at once\n",
    "    def _load_data_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Lazy load the data from file and preprocess it.\n",
    "        \"\"\"\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "    \n",
    "            ds = self._apply_scaling(ds)\n",
    "                \n",
    "            data = np.swapaxes(ds.to_array().values,0,1)  # Replace 'forecast' with the relevant key in your dataset\n",
    "    \n",
    "            # Apply augmentation if necessary\n",
    "            if self.augmentation:\n",
    "                data = self._augment_data(data)\n",
    "    \n",
    "        return torch.clamp(torch.tensor(data, dtype=torch.float32),min=0, max=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load data lazily and cache it, based on global index.\n",
    "        \"\"\"\n",
    "        # Determine which file and sample this index belongs to\n",
    "        cumulative_len = 0\n",
    "        for file_idx, path in enumerate(self.file_paths):\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                file_len = ds.sizes['time']  # Length along the 'time' dimension\n",
    "                if cumulative_len + file_len > idx:\n",
    "                    sample_idx = idx - cumulative_len\n",
    "                    data = self._load_data_from_file(path)  # Load data from cache or disk\n",
    "                    return data[sample_idx]\n",
    "                cumulative_len += file_len\n",
    "\n",
    "        raise IndexError(f\"Index {idx} is out of bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad91b6a-c202-46ad-addd-0e5a9ffcf25a",
   "metadata": {},
   "source": [
    "## Load this shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946d49b-eb66-483a-a784-9bb3e7166321",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPs = sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.nc'))\n",
    "config = get_config()\n",
    "with open('scaling_dict.pkl', 'rb') as file:\n",
    "    loaded_mean_std_dict = pickle.load(file)\n",
    "\n",
    "with open('scaling_dict_minmax.pkl', 'rb') as file:\n",
    "    loaded_min_max_dict = pickle.load(file)\n",
    "\n",
    "DP =  DataProcessed(FPs, config, loaded_mean_std_dict, loaded_min_max_dict)\n",
    "DP.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c97a3-e0ca-498e-af76-217ffecee918",
   "metadata": {},
   "source": [
    "## Lets add the Climo for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21076649-0091-4c6a-8ac5-f18572ba68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for momo in range(1,13):\n",
    "    for ii in range(1,11):\n",
    "        print(ii)\n",
    "        DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.{ii:03}.*.nc')))\n",
    "        Bingo = DSall['TREFHT']\n",
    "        Bingo[Bingo['time.month']==momo].values\n",
    "\n",
    "        #concat these all together on dimension for each month (1-12, and save each one as a netcdf file that has the lat lon info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff33de-e46e-4950-b7fa-50b4f3c33afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "# Define the output directory for the climatology files\n",
    "output_dir = \"/glade/derecho/scratch/wchapman/CESM_LE2_vars/CESM_LE2_climo/\"\n",
    "\n",
    "# Loop through each month (1 for January, 2 for February, etc.)\n",
    "for momo in range(1, 13):\n",
    "    monthly_climo = []  # List to hold all the data for this month across different ensembles/years\n",
    "    \n",
    "    # Loop through the different ensemble members (1 to 10)\n",
    "    for ii in range(1, 11):\n",
    "        print(f'{ii} ensemble')\n",
    "        # Open all the datasets for this ensemble member\n",
    "        DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.{ii:03}.*.nc')), combine='by_coords')\n",
    "        \n",
    "        # Select only the TREFHT data\n",
    "        Bingo = DSall['TREFHT']\n",
    "\n",
    "        mdo = momo + 1\n",
    "        if mdo == 13:\n",
    "            mdo = 1\n",
    "        \n",
    "        # Select only the data for the current month\n",
    "        monthly_data = Bingo.where(Bingo['time.month'] == mdo, drop=True)\n",
    "        \n",
    "        # Append this ensemble member's monthly data to the list\n",
    "        monthly_climo.append(monthly_data)\n",
    "\n",
    "    # Concatenate the data along the time dimension\n",
    "    monthly_climo_concat = xr.concat(monthly_climo, dim='time')\n",
    "    monthly_climo_concat = monthly_climo_concat.mean('time')\n",
    "    # Save the concatenated data for this month to a NetCDF file\n",
    "    output_file = f\"{output_dir}climo_month_{momo:02}.nc\"\n",
    "    monthly_climo_concat.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Saved climatology for month {momo} to {output_file}\")\n",
    "\n",
    "\n",
    "# Define the directory containing the climatology files\n",
    "climo_dir = output_dir\n",
    "\n",
    "# List to store all the monthly climatology datasets\n",
    "monthly_files = sorted(glob.glob(f'{climo_dir}climo_month_*.nc'))\n",
    "\n",
    "# Open all the monthly climatology files\n",
    "monthly_datasets = [xr.open_dataset(month) for month in monthly_files]\n",
    "\n",
    "# Concatenate the monthly datasets along a new 'time' dimension\n",
    "# We are assuming that each file represents a specific month (1 for Jan, 2 for Feb, etc.)\n",
    "climo_all_months = xr.concat(monthly_datasets, dim='time')\n",
    "\n",
    "# Assign the 'time' dimension to represent the 12 months\n",
    "climo_all_months['time'] = range(1, 13)\n",
    "\n",
    "# Save the final climatology file with all months\n",
    "output_file = f\"{climo_dir}CESM_LE2_climo_all_months.nc\"\n",
    "climo_all_months.to_netcdf(output_file)\n",
    "\n",
    "print(f\"Saved full climatology file with 12 months to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac1e28-233f-403f-b64d-b1bd9dc1ae99",
   "metadata": {},
   "source": [
    "## Add Climo to each times step in the files... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c87c47-0ded-476d-8703-e8f3ad1a5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/glade/derecho/scratch/wchapman/CESM_LE2_vars_with_climo/'\n",
    "mon_clim = xr.open_dataset('/glade/derecho/scratch/wchapman/CESM_LE2_vars/CESM_LE2_climo/CESM_LE2_climo_all_months.nc')\n",
    "\n",
    "FPS = sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars/*.nc'))\n",
    "\n",
    "for fp in FPS: \n",
    "    print(fp)\n",
    "    DS  = xr.open_dataset(fp)\n",
    "    DS['CLIM_T2M'] = xr.zeros_like(DS['TREFHT'])\n",
    "    for ii in range(len(DS['time'])):\n",
    "        monget = np.array(DS['time.month'][ii])\n",
    "        monget = monget -1\n",
    "\n",
    "        if monget == 0:\n",
    "            monget = 12\n",
    "            \n",
    "        DS['CLIM_T2M'][ii,:,:] = mon_clim.sel(time=monget)['TREFHT'].values\n",
    "    DS.to_netcdf(output_path+fp.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabb238-5c63-44f1-83ef-6938ca5df0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DSd = xr.open_dataset('/glade/derecho/scratch/wchapman/CESM_LE2_vars_with_climo//b.e21.BHISTcmip6.f09_g17.LE2-1181.010.cam.h0.PS.PRECT.TREFHT.199001-199912.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb4e00-fc47-439e-9297-90acec830ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DSd['CLIM_T2M'].isel(time=11)-climo_all_months.sel(time=12)['TREFHT']).plot(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f4735-7ad7-4e2b-9e16-1e201315cece",
   "metadata": {},
   "source": [
    "## Conditional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e38e73-dd27-4889-86af-577d59d23aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessed(Dataset):\n",
    "    def __init__(self, file_paths, config, mean_std_dict, min_max_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of paths to netCDF files\n",
    "            config: Configuration dict with scaling and augmentation options\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.config = config\n",
    "        self.loaded_mean_std_dict = mean_std_dict\n",
    "        self.loaded_min_max_dict = min_max_dict\n",
    "        self.augmentation = config.get('augment', False)\n",
    "        self.scaler = None\n",
    "        if config['scaling'] == 'quantile':\n",
    "            self.scaler = QuantileTransformer()\n",
    "\n",
    "        # Initialize cache size (you can adjust it depending on memory constraints)\n",
    "        self.cache_size = config.get('cache_size', 10)\n",
    "        self.cached_data = {}  # Manual cache for storing loaded data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate total number of samples across all files\n",
    "        total_len = 0\n",
    "        for path in self.file_paths:\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                total_len += ds.sizes['time']  # Assuming 'time' is the main dimension\n",
    "        return total_len\n",
    "\n",
    "    def _apply_scaling(self, ds):\n",
    "        if self.config['scaling'] == 'std':\n",
    "            ds['PS'][:,:,:] = (ds['PS'][:,:,:] - self.loaded_mean_std_dict['PS_mean']) / self.loaded_mean_std_dict['PS_std']\n",
    "            ds['PRECT'][:,:,:] = (ds['PRECT'][:,:,:] - self.loaded_mean_std_dict['PRECT_mean']) / self.loaded_mean_std_dict['PRECT_std']\n",
    "            ds['TREFHT'][:,:,:] = (ds['TREFHT'][:,:,:] - self.loaded_mean_std_dict['TREFHT_mean']) / self.loaded_mean_std_dict['TREFHT_std']\n",
    "    \n",
    "            vars = ['PS','PRECT','TREFHT']\n",
    "    \n",
    "            for vv in vars:\n",
    "                mini = self.loaded_min_max_dict[f'{vv}_min']\n",
    "                maxi = self.loaded_min_max_dict[f'{vv}_max']\n",
    "                if maxi > 10:\n",
    "                    maxi = 10\n",
    "                if mini <-10:\n",
    "                    mini = 10\n",
    "                \n",
    "                ds[vv][:,:,:] =  (ds[vv][:,:,:] - mini) / (maxi - mini)\n",
    "\n",
    "            ds['CLIM_T2M'][:,:,:] = (ds['CLIM_T2M'][:,:,:] - 211.45393372)/(313.99099731-211.45393372)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method specified.\")\n",
    "    \n",
    "        return ds\n",
    "\n",
    "    def _augment_data(self, data):\n",
    "        # Apply rotations for augmentation\n",
    "        data_rot90 = np.rot90(data, k=1, axes=(1, 2))\n",
    "        data_rot180 = np.rot90(data, k=2, axes=(1, 2))\n",
    "        data_rot270 = np.rot90(data, k=3, axes=(1, 2))\n",
    "        \n",
    "        # Concatenate all rotations along the first dimension (time)\n",
    "        return np.concatenate([data, data_rot90, data_rot180, data_rot270], axis=0)\n",
    "\n",
    "    @lru_cache(maxsize=2)  # Cache up to 5 file loads at once\n",
    "    def _load_data_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Lazy load the data from file and preprocess it.\n",
    "        \"\"\"\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "    \n",
    "            ds = self._apply_scaling(ds)\n",
    "            \n",
    "                \n",
    "            data = np.swapaxes(ds[['PS','PRECT','TREFHT']].to_array().values,0,1)  # Replace 'forecast' with the relevant key in your dataset\n",
    "            cond = np.expand_dims(ds['CLIM_T2M'].values,1)  # Replace 'forecast' with the relevant key in your dataset\n",
    "    \n",
    "            # Apply augmentation if necessary\n",
    "            if self.augmentation:\n",
    "                data = self._augment_data(data)\n",
    "    \n",
    "        return torch.clamp(torch.tensor(data, dtype=torch.float32),min=0, max=1), torch.clamp(torch.tensor(cond, dtype=torch.float32),min=0, max=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load data lazily and cache it, based on global index.\n",
    "        \"\"\"\n",
    "        # Determine which file and sample this index belongs to\n",
    "        cumulative_len = 0\n",
    "        for file_idx, path in enumerate(self.file_paths):\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                file_len = ds.sizes['time']  # Length along the 'time' dimension\n",
    "                if cumulative_len + file_len > idx:\n",
    "                    sample_idx = idx - cumulative_len\n",
    "                    data, cond = self._load_data_from_file(path)  # Load data from cache or disk\n",
    "                    return data[sample_idx], cond[sample_idx]\n",
    "                cumulative_len += file_len\n",
    "\n",
    "        raise IndexError(f\"Index {idx} is out of bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37355d0-728a-4990-9251-a56b489d127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPs = sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars_with_climo/*.nc'))\n",
    "config = get_config()\n",
    "with open('scaling_dict.pkl', 'rb') as file:\n",
    "    loaded_mean_std_dict = pickle.load(file)\n",
    "\n",
    "with open('scaling_dict_minmax.pkl', 'rb') as file:\n",
    "    loaded_min_max_dict = pickle.load(file)\n",
    "\n",
    "DP =  DataProcessed(FPs, config, loaded_mean_std_dict, loaded_min_max_dict)\n",
    "DP.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26837d14-e355-4b33-ab62-3040c580089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(DP, batch_size = 4, shuffle = False, pin_memory = True, num_workers = 1)\n",
    "dd = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ec24c-eada-41db-9bbe-2d14250a0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "word_site = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "response = requests.get(word_site)\n",
    "WORDS = response.content.splitlines()\n",
    "\n",
    "# Generate random indices\n",
    "rn1 = random.randint(0, len(WORDS) - 1)\n",
    "rn2 = random.randint(0, len(WORDS) - 1)\n",
    "\n",
    "# Decode the byte lines to strings\n",
    "w1 = WORDS[rn1].decode('utf-8')\n",
    "w2 = WORDS[rn2].decode('utf-8')\n",
    "\n",
    "print(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bf930-0d1e-4fa4-8ae5-5abe4ca29f2f",
   "metadata": {},
   "source": [
    "## Show regression maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e9757fd3-8e24-4270-b06b-f2ce6e0dd238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessed(Dataset):\n",
    "    def __init__(self, file_paths, config, mean_std_dict, min_max_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of paths to netCDF files\n",
    "            config: Configuration dict with scaling and augmentation options\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.config = config\n",
    "        self.loaded_mean_std_dict = mean_std_dict\n",
    "        self.loaded_min_max_dict = min_max_dict\n",
    "        self.augmentation = config.get('augment', False)\n",
    "        self.scaler = None\n",
    "        if config['scaling'] == 'quantile':\n",
    "            self.scaler = QuantileTransformer()\n",
    "\n",
    "        # Initialize cache size (you can adjust it depending on memory constraints)\n",
    "        self.cache_size = config.get('cache_size', 10)\n",
    "        self.cached_data = {}  # Manual cache for storing loaded data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate total number of samples across all files\n",
    "        total_len = 0\n",
    "        for path in self.file_paths:\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                total_len += ds.sizes['time']  # Assuming 'time' is the main dimension\n",
    "        return total_len\n",
    "\n",
    "    def _apply_scaling(self, ds):\n",
    "        if self.config['scaling'] == 'std':\n",
    "            ds['PS'][:,:,:] = (ds['PS'][:,:,:] - self.loaded_mean_std_dict['PS_mean']) / self.loaded_mean_std_dict['PS_std']\n",
    "            ds['PRECT'][:,:,:] = (ds['PRECT'][:,:,:] - self.loaded_mean_std_dict['PRECT_mean']) / self.loaded_mean_std_dict['PRECT_std']\n",
    "            ds['TREFHT'][:,:,:] = (ds['TREFHT'][:,:,:] - self.loaded_mean_std_dict['TREFHT_mean']) / self.loaded_mean_std_dict['TREFHT_std']\n",
    "    \n",
    "            vars = ['PS','PRECT','TREFHT']\n",
    "    \n",
    "            for vv in vars:\n",
    "                mini = self.loaded_min_max_dict[f'{vv}_min']\n",
    "                maxi = self.loaded_min_max_dict[f'{vv}_max']\n",
    "                if maxi > 10:\n",
    "                    maxi = 10\n",
    "                if mini <-10:\n",
    "                    mini = 10\n",
    "                \n",
    "                ds[vv][:,:,:] =  (ds[vv][:,:,:] - mini) / (maxi - mini)\n",
    "\n",
    "            ds['month_expanded']=ds['month_expanded'].copy()\n",
    "            ds['co2vmr_expanded']=ds['co2vmr_expanded'].copy()\n",
    "\n",
    "            print(np.max(ds['month_expanded']))\n",
    "\n",
    "            ds['month_expanded_scaled'] = (ds['month_expanded'][:,:,:] - 1)/(12-1)\n",
    "            ds['co2vmr_expanded_scaled'] = (ds['co2vmr_expanded'][:,:,:] - 0.00039895)/(0.0008223-0.00039895)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method specified.\")\n",
    "    \n",
    "        return ds\n",
    "\n",
    "    def _unapply_scaling(self, ds):\n",
    "        if self.config['scaling'] == 'std':\n",
    "            # Reverse the min-max scaling for 'PS', 'PRECT', and 'TREFHT'\n",
    "            vars = ['PS', 'PRECT', 'TREFHT']\n",
    "            \n",
    "            for vv in vars:\n",
    "                mini = self.loaded_min_max_dict[f'{vv}_min']\n",
    "                maxi = self.loaded_min_max_dict[f'{vv}_max']\n",
    "                \n",
    "                if maxi > 10:\n",
    "                    maxi = 10\n",
    "                if mini < -10:\n",
    "                    mini = -10\n",
    "                \n",
    "                # Reverse the min-max scaling\n",
    "                ds[vv][:,:,:] = ds[vv][:,:,:] * (maxi - mini) + mini\n",
    "    \n",
    "            # Reverse the standardization (z-score normalization) for 'PS', 'PRECT', and 'TREFHT'\n",
    "            ds['PS'][:,:,:] = ds['PS'][:,:,:] * self.loaded_mean_std_dict['PS_std'] + self.loaded_mean_std_dict['PS_mean']\n",
    "            ds['PRECT'][:,:,:] = ds['PRECT'][:,:,:] * self.loaded_mean_std_dict['PRECT_std'] + self.loaded_mean_std_dict['PRECT_mean']\n",
    "            ds['TREFHT'][:,:,:] = ds['TREFHT'][:,:,:] * self.loaded_mean_std_dict['TREFHT_std'] + self.loaded_mean_std_dict['TREFHT_mean']\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method specified.\")\n",
    "    \n",
    "        return ds\n",
    "\n",
    "    def _augment_data(self, data):\n",
    "        # Apply rotations for augmentation\n",
    "        data_rot90 = np.rot90(data, k=1, axes=(1, 2))\n",
    "        data_rot180 = np.rot90(data, k=2, axes=(1, 2))\n",
    "        data_rot270 = np.rot90(data, k=3, axes=(1, 2))\n",
    "        \n",
    "        # Concatenate all rotations along the first dimension (time)\n",
    "        return np.concatenate([data, data_rot90, data_rot180, data_rot270], axis=0)\n",
    "\n",
    "    @lru_cache(maxsize=2)  # Cache up to 5 file loads at once\n",
    "    def _load_data_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Lazy load the data from file and preprocess it.\n",
    "        \"\"\"\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "\n",
    "            # Get the shape of the other variables (time, lat, lon)\n",
    "            lat = ds['PS'].coords['lat']\n",
    "            lon = ds['PS'].coords['lon']\n",
    "            \n",
    "            # Broadcast co2vmr across (time, lat, lon)\n",
    "            co2vmr_broadcasted = ds['co2vmr'].broadcast_like(ds['PS'])\n",
    "            \n",
    "            # Alternatively, you can expand the dimensions manually\n",
    "            co2vmr_expanded = ds['co2vmr'].expand_dims({'lat': lat, 'lon': lon}, axis=(1, 2))\n",
    "            \n",
    "            # Assign this expanded co2vmr back to the dataset with the correct dimensions\n",
    "            ds['co2vmr_expanded'] = co2vmr_expanded\n",
    "            \n",
    "            ds['time.month'].broadcast_like(ds['PS'])\n",
    "            # Alternatively, you can expand the dimensions manually\n",
    "            month_expanded = ds['time.month'].expand_dims({'lat': lat, 'lon': lon}, axis=(1, 2))\n",
    "            month_expanded = month_expanded-1\n",
    "            month_corrected = month_expanded.where(month_expanded != 0, 12)\n",
    "            ds['month_expanded'] = month_corrected\n",
    "            print(np.max(ds['month_expanded']))    \n",
    "            ds = self._apply_scaling(ds)\n",
    "            data = np.swapaxes(ds[['PS','PRECT','TREFHT']].to_array().values,0,1)  # Replace 'forecast' with the relevant key in your dataset\n",
    "            cond = np.swapaxes(ds[['month_expanded_scaled','co2vmr_expanded_scaled']].to_array().values,0,1)  # Replace 'forecast' with the relevant key in your dataset\n",
    "    \n",
    "            # Apply augmentation if necessary\n",
    "            if self.augmentation:\n",
    "                data = self._augment_data(data)\n",
    "    \n",
    "        return torch.clamp(torch.tensor(data, dtype=torch.float32),min=0, max=1), torch.clamp(torch.tensor(cond, dtype=torch.float32),min=0, max=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load data lazily and cache it, based on global index.\n",
    "        \"\"\"\n",
    "        # Determine which file and sample this index belongs to\n",
    "        cumulative_len = 0\n",
    "        for file_idx, path in enumerate(self.file_paths):\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                file_len = ds.sizes['time']  # Length along the 'time' dimension\n",
    "                if cumulative_len + file_len > idx:\n",
    "                    sample_idx = idx - cumulative_len\n",
    "                    data, cond = self._load_data_from_file(path)  # Load data from cache or disk\n",
    "                    return data[sample_idx], cond[sample_idx]\n",
    "                cumulative_len += file_len\n",
    "\n",
    "        raise IndexError(f\"Index {idx} is out of bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5fbb64b5-8233-40f0-9143-a48d4f93e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"input_channels\": 1,\n",
    "        \"output_channels\": 1,\n",
    "        \"context_image\": True,\n",
    "        \"context_channels\": 1,\n",
    "        \"num_blocks\": [2, 2],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"hidden_context_channels\": 8,\n",
    "        \"time_embedding_dim\": 256,\n",
    "        \"image_size\": 128,\n",
    "        \"noise_sampling_coeff\": 0.85,\n",
    "        \"denoise_time\": 970,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm\": True,\n",
    "        \"subsample\": 100000,\n",
    "        \"save_name\": \"model_weights.pt\",\n",
    "        \"dim_mults\": (2, 4, 6, 8),\n",
    "        \"flash_attn\": True,\n",
    "        \"base_dim\": 32,\n",
    "        \"timesteps\": 1000,\n",
    "        \"pading\": \"reflect\",\n",
    "        \"scaling\": \"std\",\n",
    "        \"batch_size\" : 44,\n",
    "        \"dropout\": 0.,\n",
    "        \"optimization\": {\n",
    "            \"epochs\": 400,\n",
    "            \"lr\": 0.01,\n",
    "            \"wd\": 0.05,\n",
    "            \"scheduler\": True\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "folder = '/glade/derecho/scratch/wchapman/CESM_LE2_vars_BSSP370cmip6/'\n",
    "FPs = sorted(glob.glob(f'/{folder}/*.nc'))\n",
    "with open('scaling_dict_CC.pkl', 'rb') as file:\n",
    "    loaded_mean_std_dict = pickle.load(file)\n",
    "        \n",
    "with open('scaling_dict_minmax_CC.pkl', 'rb') as file:\n",
    "    loaded_min_max_dict = pickle.load(file)\n",
    "\n",
    "ds = DataProcessed(FPs, config, loaded_mean_std_dict, loaded_min_max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50deeb45-21fc-40bd-a47c-9cd513c508a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4902c5ae-0631-4100-99c5-7a35c1f8eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Load from the file\n",
    "ma_1 = []\n",
    "ma_2 = []\n",
    "ma_3 = []\n",
    "mi_1 = []\n",
    "mi_2 = []\n",
    "mi_3 = []\n",
    "\n",
    "with open('scaling_dict_CC.pkl', 'rb') as file:\n",
    "    loaded_dicter = pickle.load(file)\n",
    "\n",
    "for ii in range(1,11):\n",
    "    print(ii)\n",
    "    DSall = xr.open_mfdataset(sorted(glob.glob(f'/glade/derecho/scratch/wchapman/CESM_LE2_vars_BSSP370cmip6/*.{ii:03}.*.nc')))\n",
    "\n",
    "    ma_1.append(np.max(DSall['co2vmr']).values)\n",
    "    mi_1.append(np.min(DSall['co2vmr']).values)\n",
    "\n",
    "# dicter = {'PS_max':np.array(ma_1).max(),'PRECT_max':np.array(ma_2).max(),'TREFHT_max':np.array(ma_3).max(),\n",
    "#           'PS_min':np.array(mi_1).min(),'PRECT_min':np.array(mi_2).min(),'TREFHT_min':np.array(mi_3).min()}\n",
    "# # Save to a file\n",
    "# with open('scaling_dict_minmax_CC.pkl', 'wb') as file:\n",
    "#     pickle.dump(dicter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f0837-a5f5-44f2-a7e6-349c63ae5ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafd5f1-4f2a-4a0d-968c-82050cad427d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e4acb-a250-4088-9ef8-94fe7ba00682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757889ae-24af-4844-8f34-67719e948599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fcb2b273-f97a-47bc-9f8c-b378fc007a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the other variables (time, lat, lon)\n",
    "lat = DS['PS'].coords['lat']\n",
    "lon = DS['PS'].coords['lon']\n",
    "\n",
    "# Broadcast co2vmr across (time, lat, lon)\n",
    "co2vmr_broadcasted = DS['co2vmr'].broadcast_like(DS['PS'])\n",
    "\n",
    "# Alternatively, you can expand the dimensions manually\n",
    "co2vmr_expanded = DS['co2vmr'].expand_dims({'lat': lat, 'lon': lon}, axis=(1, 2))\n",
    "\n",
    "# Assign this expanded co2vmr back to the dataset with the correct dimensions\n",
    "DS['co2vmr_expanded'] = co2vmr_expanded\n",
    "\n",
    "DS['time.month'].broadcast_like(DS['PS'])\n",
    "# Alternatively, you can expand the dimensions manually\n",
    "month_expanded = DS['time.month'].expand_dims({'lat': lat, 'lon': lon}, axis=(1, 2))\n",
    "month_expanded = month_expanded-1\n",
    "month_corrected = month_expanded.where(month_expanded != 0, 12)\n",
    "\n",
    "DS['month_expanded'] = month_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1cdb9-52a0-4ba5-bbe3-bc10a9576520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LuRain)",
   "language": "python",
   "name": "lurain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
