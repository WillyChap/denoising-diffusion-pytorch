{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef4d710-37da-40c1-8967-1a2692fabcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58eb525f-7b6d-41ed-a4fe-b13a934284ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd306d-4d9d-4d12-aae3-71a2ef898fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"input_channels\": 1,\n",
    "        \"output_channels\": 1,\n",
    "        \"context_image\": True,\n",
    "        \"context_channels\": 1,\n",
    "        \"num_blocks\": [2, 2],\n",
    "        \"hidden_channels\": 32,\n",
    "        \"hidden_context_channels\": 8,\n",
    "        \"time_embedding_dim\": 256,\n",
    "        \"image_size\": 128,\n",
    "        \"noise_sampling_coeff\": 0.85,\n",
    "        \"denoise_time\": 970,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm\": True,\n",
    "        \"subsample\": 100000,\n",
    "        \"save_name\": \"model_weights.pt\",\n",
    "        \"dim_mults\": [4, 4],\n",
    "        \"base_dim\": 32,\n",
    "        \"timesteps\": 1000,\n",
    "        \"pading\": \"reflect\",\n",
    "        \"scaling\": \"std\",\n",
    "        \"optimization\": {\n",
    "            \"epochs\": 400,\n",
    "            \"lr\": 0.01,\n",
    "            \"wd\": 0.05,\n",
    "            \"batch_size\": 32,\n",
    "            \"scheduler\": True\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def load_datasets(config):\n",
    "    lead_nums = [str(int(i)) for i in np.linspace(0, 168, 57)]\n",
    "    \n",
    "    # Load datasets using xarray and convert directly to NumPy arrays\n",
    "    train_datasets = [xr.open_dataset(f'/glade/derecho/scratch/timothyh/data/diffusion_forecasts/processed/lead_{i}/train.nc') for i in lead_nums]\n",
    "    test_datasets = [xr.open_dataset(f'/glade/derecho/scratch/timothyh/data/diffusion_forecasts/processed/lead_{i}/test.nc') for i in lead_nums]\n",
    "    val_datasets = [xr.open_dataset(f'/glade/derecho/scratch/timothyh/data/diffusion_forecasts/processed/lead_{i}/val.nc') for i in lead_nums]\n",
    "\n",
    "    # Concatenate forecasts along the time dimension and convert to NumPy arrays\n",
    "    xtrain = np.concatenate([ds.forecast.values for ds in train_datasets], axis=0)\n",
    "    xtest = np.concatenate([ds.forecast.values for ds in test_datasets], axis=0)\n",
    "    xval = np.concatenate([ds.forecast.values for ds in val_datasets], axis=0)\n",
    "\n",
    "    # Load the Quantile Transform Scaler\n",
    "    if config['scaling']=='quantile':\n",
    "        print('quantiling')\n",
    "        qtpie = load(open('quantile_transform_scaler.pkl', 'rb'))\n",
    "    \n",
    "        print('... transforming ...')\n",
    "        # Apply Quantile Normalization\n",
    "        xtrain_t = qtpie.transform(xtrain.reshape(-1, 1)).reshape(xtrain.shape)\n",
    "        print('... done 1 ...')\n",
    "        xtest_t = qtpie.transform(xtest.reshape(-1, 1)).reshape(xtest.shape)\n",
    "        xval_t = qtpie.transform(xval.reshape(-1, 1)).reshape(xval.shape)\n",
    "    elif config['scaling']=='std':\n",
    "        print('standardizing')\n",
    "        meanx = np.mean(xtrain)\n",
    "        stdx = np.std(xtrain)\n",
    "\n",
    "        xtrain_t = (xtrain-meanx)/stdx\n",
    "        xval_t = (xval-meanx)/stdx\n",
    "        xtest_t = (xtest-meanx)/stdx\n",
    "        config['mean'] = meanx\n",
    "        config['std'] = stdx\n",
    "    else:\n",
    "        print('bad things happened')\n",
    "        raise \n",
    "\n",
    "    print('... rotating ...')\n",
    "\n",
    "    # Data Augmentation using NumPy (rotations)\n",
    "    xtrain_rot90 = np.rot90(xtrain_t, k=1, axes=(1, 2))  # 90 degrees rotation\n",
    "    xtrain_rot180 = np.rot90(xtrain_t, k=2, axes=(1, 2)) # 180 degrees rotation\n",
    "    xtrain_rot270 = np.rot90(xtrain_t, k=3, axes=(1, 2)) # 270 degrees rotation\n",
    "\n",
    "    # Concatenate all rotations along the first dimension (time)\n",
    "    xtrain_all = np.concatenate([xtrain_t, xtrain_rot90, xtrain_rot180, xtrain_rot270], axis=0)\n",
    "\n",
    "    # Create datasets as NumPy arrays\n",
    "    train_dataset = DataProcessed(xtrain_all)\n",
    "    test_dataset = DataProcessed(xtest_t)\n",
    "    val_dataset = DataProcessed(xval_t)\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2f857b-e008-4049-a41e-01284a6318ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_nums = [str(int(i)) for i in np.linspace(0, 168, 57)]\n",
    "train_datasets = [(f'/glade/derecho/scratch/timothyh/data/diffusion_forecasts/processed/lead_{i}/train.nc') for i in lead_nums]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0354099b-c597-4b13-9433-ca0989882d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessed(Dataset):\n",
    "    def __init__(self, file_paths, config, mean, std):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of paths to netCDF files\n",
    "            config: Configuration dict with scaling and augmentation options\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.config = config\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.augmentation = config.get('augment', False)\n",
    "        self.scaler = None\n",
    "        if config['scaling'] == 'quantile':\n",
    "            self.scaler = QuantileTransformer()\n",
    "\n",
    "         # Initialize cache size (you can adjust it depending on memory constraints)\n",
    "        self.cache_size = config.get('cache_size', 10)\n",
    "        self.cached_data = {}  # Manual cache for storing loaded data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate total number of samples across all files\n",
    "        total_len = 0\n",
    "        for path in self.file_paths:\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                total_len += ds.sizes['time']  # Assuming 'time' is the main dimension\n",
    "        return total_len\n",
    "\n",
    "    def _apply_scaling(self, data):\n",
    "        if self.config['scaling'] == 'quantile':\n",
    "            data_t = self.scaler.fit_transform(data.reshape(-1, 1)).reshape(data.shape)\n",
    "        elif self.config['scaling'] == 'std':\n",
    "            data_t = (data - self.mean) / self.std\n",
    "            self.config['mean'], self.config['std'] = self.mean, self.std\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method specified.\")\n",
    "        return data_t\n",
    "\n",
    "    def _augment_data(self, data):\n",
    "        # Apply rotations for augmentation\n",
    "        data_rot90 = np.rot90(data, k=1, axes=(1, 2))\n",
    "        data_rot180 = np.rot90(data, k=2, axes=(1, 2))\n",
    "        data_rot270 = np.rot90(data, k=3, axes=(1, 2))\n",
    "        \n",
    "        # Concatenate all rotations along the first dimension (time)\n",
    "        return np.concatenate([data, data_rot90, data_rot180, data_rot270], axis=0)\n",
    "\n",
    "    @lru_cache(maxsize=2)  # Cache up to 5 file loads at once\n",
    "    def _load_data_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Lazy load the data from file and preprocess it.\n",
    "        \"\"\"\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            data = ds.forecast.values  # Replace 'forecast' with the relevant key in your dataset\n",
    "\n",
    "            # Apply scaling\n",
    "            data = self._apply_scaling(data)\n",
    "\n",
    "            # Apply augmentation if necessary\n",
    "            if self.augmentation:\n",
    "                data = self._augment_data(data)\n",
    "\n",
    "        return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load data lazily and cache it, based on global index.\n",
    "        \"\"\"\n",
    "        # Determine which file and sample this index belongs to\n",
    "        cumulative_len = 0\n",
    "        for file_idx, path in enumerate(self.file_paths):\n",
    "            with xr.open_dataset(path) as ds:\n",
    "                file_len = ds.sizes['time']  # Length along the 'time' dimension\n",
    "                if cumulative_len + file_len > idx:\n",
    "                    sample_idx = idx - cumulative_len\n",
    "                    data = self._load_data_from_file(path)  # Load data from cache or disk\n",
    "                    return data[sample_idx]\n",
    "                cumulative_len += file_len\n",
    "\n",
    "        raise IndexError(f\"Index {idx} is out of bounds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebf72d4-7c09-439d-b93c-750cde982e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_ds = DataProcessed(file_paths=train_datasets, config=config, mean=152, std=152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c26547a-0f6d-41ef-92eb-85071eab11a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215503"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af16ba9f-fc47-4e1e-9fc7-20930b6177bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/conda-envs/therm_diff/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=config[\"optimization\"][\"batch_size\"], shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be06b9-cb11-47e1-89b7-adc5d338e31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (therm_diff)",
   "language": "python",
   "name": "therm_diff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
